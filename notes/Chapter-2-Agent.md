# Chapter 2 Agent

>- 对应于 "Artificial Intelligence - A Modern Approach (3rd Edition)" 中的 Chapter 2
>- INTELLIGENT AGENT (2.1 ~ 2.5)

人工智能是通过机器展现出来的智能。在计算机科学中， 人工智能研究领域将其自身定义为对 “智能 Agent ”的研究：任何 _感知到环境_ 并采取行动以 _最大化其成功机会_ 的设备

## Agent和环境

Agent = 体系结构 + Agent程序

- Agent函数f：将 _感知序列_ 映射到 _行动_
- Agent程序：实现Agent函数f的执行
- Agent体系结构：Agent程序需要在某个具备物理 _传感器和执行器_ 的计算装置上运行

## Agent函数和程序

- Agent完全由Agent函数将感知序列映射到行动来指定
- Agent函数是理性的
- AI的目的：找到一种简洁实现理性Agent函数的方法


- 理性Agent是做正确的事的Agent——从概念上讲， Agent 函数表格的每一项都填写正确
- 性能度量：对 __环境状态__ 的任何给定序列进行评估（环境状态，不是Agent状态）


任何指定的时刻，判断理性依赖于四个方面:

- 定义成功标准的 _性能度量_ 
- Agent对 _环境的先验_ 知识
- Agent _可以完成的行动_ 
- Agent截止到此时的 _感知序列_ 


理性Agent的定义：

对每一个可能的感知序列， 根据 _已知的感知序列_ 提供的证据和Agent具有的 _先验知识_ ，理性Agent应该选择能够使其 _性能度量最大化的行动_ 

## 任务环境

要构建一个理性 Agent ，首先必须指定任务环境：PEAS (Performance, Environment, Actuators, Sensors)

### 环境类型

>- 环境类型在很大程度上决定了Agent设计
>- 现实世界始终是部分可观察的、随机的、延续的、动态 的、连续的和多Agent的

#### 完全可观察的 vs. 部分可观察的

完全可观察的: Agent的传感器在 _每个时间点_ 上都能获取 _环境的完整状态_ 

部分可观察的: 噪声、不精确的传感器，或者传感器丢失了部分状态数据，都可能导致环境成为部分可观察的

#### 单Agent vs. 多Agent

单Agent: 在环境中 __自主运行__ 的Agent

多Agent: 一个Agent的行为是否寻求让依赖于另一个 Agent的行为性能度量最大化

#### 确定的 vs. 随机的

确定的: 环境的下一状态完全取决于当前状态和Agent 执行的动作。Agent在完全可观察的、确定的环境中无需考虑不确定性

随机的: 如果环境是部分可观察的，那么它可能表现为随机的

#### 片段式的 vs. 延续式的

在片段式的任务环境中，Agent的经历被分成多个原子 片段。每个原子片段中Agent感知信息并完成单个行动， 原子片段之间无依赖

在延续式环境中，当前决策会影响所有未来的决策

#### 静态的 vs. 动态的

如果环境在Agent计算时会变化，那么是动态的，反之是静态的

如果环境本身不随时间变化，但Agent性能评价随时间动态变化，那么是半动态的

#### 离散的 vs. 连续的

环境的状态、时间的处理方式以及Agent的感知信息和 行动，都有离散和连续之分

#### 已知的 vs. 未知的

在已知环境中，所有行动的后果是给定的

在未知环境中，Agent需要学习环境是如何工作的，以便做出好的决策

## Agent类型

1. 简单反射Agent  
	1. 动作完全取决于条件-行为规则: 如果满足条件，则采取动作
	2. 也称为无内存或无状态。当仅 基于当前的感知做出正确的决定时，才有效
	3. 仅在完全可观察的环境下有效。 如果环境是部分可观察的，则通常陷入无限循环
2. 基于模型的Agent  
	1. 维持内部状态来对世界进行建模
	2. 世界模型代表Agent的最佳猜测(或预测)，不完全准确
	3. 内部状态也可以用于维护Agent的状态，而不是维护整个世界的状态
3. 基于目标的Agent
	1. Agent不使用条件行为规则，而是使用目标来决定其执行的操作
4. 基于效用的Agent
	1. 效用函数: Agent的快乐度
	2. 最大化期望效用


所有这些Agent均可以转换为 __学习Agent__！


学习型Agent

- 从奖励(或惩罚)中学习
- 学习技术形成了机器学习领域

